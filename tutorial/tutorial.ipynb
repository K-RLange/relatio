{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ee00a3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# An introduction to `relatio` \n",
    "**Runtime $\\sim$ 20min**\n",
    "\n",
    "----------------------------\n",
    "\n",
    "This is a short demo of the package `relatio`.  It takes as input a text corpus and outputs a list of narrative statements. The pipeline is unsupervised: the user does not need to specify narratives beforehand. Narrative statements are defined as tuples of semantic roles with a (agent, verb, patient, attribute) structure. \n",
    "\n",
    "Here, we present the main wrapper functions to quickly obtain narrative statements from a corpus.\n",
    "\n",
    "For further details, please refer to the paper: [\"Text Semantics Capture Political and Economic Narratives\"](https://arxiv.org/abs/2108.01720)\n",
    "\n",
    "----------------------------\n",
    "\n",
    "We provide datasets that have already been split into sentences and annotated by our team.\n",
    "\n",
    "The datasets are provided in three different formats:\n",
    " 1. `raw` (unprocessed)\n",
    " 2. `split_sentences` (as a list of sentences)\n",
    " 3. `srl` (as a list of annotated sentences by the semantic role labeler)\n",
    "\n",
    "In this tutorial, we work with the Trump Tweet Archive corpus.\n",
    "\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045e381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of available datasets\n",
    "\n",
    "from relatio.datasets import list_datasets\n",
    "\n",
    "print(list_datasets())\n",
    "\n",
    "# Download Trump Tweet Archive in \"raw\" format\n",
    "\n",
    "from relatio.datasets import load_trump_data\n",
    "\n",
    "df = load_trump_data(\"raw\")\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac363a9",
   "metadata": {},
   "source": [
    "## Step 1: Split into sentences\n",
    "\n",
    "----------------------------\n",
    "\n",
    "For any new corpus, the first thing you will want to do is to split the corpus into sentences.\n",
    "\n",
    "We do this on the first 100 tweets. \n",
    "\n",
    "The output is two lists: one with an index for the document and one with the resulting split sentences.\n",
    "\n",
    "----------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41183920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from relatio.utils import split_into_sentences\n",
    "\n",
    "split_sentences = split_into_sentences(\n",
    "    df.iloc[0:100], progress_bar=True\n",
    ")\n",
    "\n",
    "for i in range(5):\n",
    "    print('Document id: %s' %split_sentences[0][i])\n",
    "    print('Sentence: %s \\n' %split_sentences[1][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291b8592",
   "metadata": {},
   "source": [
    "## Step 2: Annotate semantic roles\n",
    "\n",
    "----------------------------\n",
    "\n",
    "Once the corpus is split into sentences. You can feed it to the semantic role labeler.\n",
    "\n",
    "The output is a list of json objects which contain the semantic role annotations for each sentence in the corpus.\n",
    "\n",
    "----------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a72d3df",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note that SRL is time-consuming, in particular on CPUs.\n",
    "# To speed up the annotation, you can also use GPUs via the \"cuda_device\" argument of the \"run_srl()\" function. \n",
    "\n",
    "from relatio.wrappers import run_srl\n",
    "\n",
    "srl_res = run_srl(\n",
    "    path=\"https://storage.googleapis.com/allennlp-public-models/openie-model.2020.03.26.tar.gz\", # pre-trained model\n",
    "    sentences=split_sentences[1],\n",
    "    progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3ffc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of SRL output\n",
    "\n",
    "srl_res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138321cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save us some time, we download the results from the datasets module.\n",
    "\n",
    "split_sentences = load_trump_data(\"split_sentences\")\n",
    "srl_res = load_trump_data(\"srl_res\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3e8f9f",
   "metadata": {},
   "source": [
    "## Step 3: Build the narrative model\n",
    "\n",
    "----------------------------\n",
    "\n",
    "We are now ready to build a narrative model.\n",
    "\n",
    "The function `build_narrative_model` takes as input the split sentences the SRL annotations for the corpus. It builds a model of low-dimensional narrative statements which may then be used to obtain narrative statements  \"out-of-sample\".\n",
    "\n",
    "The function has sensible defaults for most arguments, but the user should at least specify:\n",
    "- the number of latent unnamed entities to recover (`n_clusters`)\n",
    "- the embeddings to be used (see here for further details) \n",
    "\n",
    "We specify 100 unnamed entities to uncover. The embeddings used are pre-trained glove embeddings.\n",
    "\n",
    "To speed up the model's training, we also focus on the top 100 most frequent named entities (the default is to mine all named entities).\n",
    "\n",
    "To improve interpretability, we remove common uninformative words in the corpus (the \"stopwords\"), as well as one-letter words.\n",
    "\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392d7db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of stopwords from SpaCy\n",
    "\n",
    "import spacy\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# NB: This step usually takes several minutes to run. You might want to grab a coffee.\n",
    " \n",
    "from relatio.wrappers import build_narrative_model\n",
    "\n",
    "narrative_model = build_narrative_model(\n",
    "    srl_res=srl_res,\n",
    "    sentences=split_sentences[1],\n",
    "    embeddings_type=\"gensim_keyed_vectors\",  # see documentation for a list of supported types\n",
    "    embeddings_path=\"glove-wiki-gigaword-300\",\n",
    "    n_clusters=[[100]],\n",
    "    top_n_entities=100,\n",
    "    stop_words = spacy_stopwords,\n",
    "    remove_n_letter_words = 1,\n",
    "    progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e7a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The narrative model is simply a dictionary containing the narrative model's specifics.\n",
    "\n",
    "print(narrative_model.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359d6514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common named entities\n",
    "\n",
    "narrative_model['entities'].most_common()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71ea112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The unnamed entities uncovered in the corpus \n",
    "# (automatically labeled by the most frequent phrase in the cluster)\n",
    "\n",
    "narrative_model['cluster_labels_most_freq']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125f9c77",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "\n",
    "In practice, `build_narrative_model` is a flexible wrapper function which gives a lot of control to the user. \n",
    "\n",
    "Let's break the options down into four categories:\n",
    "\n",
    "1. Basic utilities\n",
    "    - the semantic roles you're interested in (`roles_considered`)\n",
    "    - where you would like to save the model (`output_path`)\n",
    "    - whether you cant to track the function's progress (`progress_bar`)\n",
    "    \n",
    "\n",
    "2. Text preprocessing\n",
    "    - basic text preprocessing steps \n",
    "    (`remove_punctuation`, `remove_digits`, remove `stop_words`, `stem` or `lemmatize` words, etc.)\n",
    "    - would you like to replace verbs by their most common synonyms/antonyms? (`dimension_reduce_verbs`)\n",
    "\n",
    "\n",
    "3. Named entities  \n",
    "    - which semantic roles have named entities? (`roles_with_entities`)\n",
    "    - how many named entities would you like to keep? (`top_n_entities`)\n",
    "\n",
    "Technical details: under the hood, we work with SpaCy named entity recognizer to identify named entities. \n",
    "We consider tags related to places, organizations, people and events.\n",
    "\n",
    "\n",
    "4. Unnamed entities (e.g., tax, government, dog, cat, etc.)\n",
    "    - which semantic roles have unnamed entities? (`roles_with_embeddings`)\n",
    "    - how many latent unnamed entities are there in the corpus? (`n_clusters`)\n",
    "\n",
    "Technical details: under the hood, we embed semantic phrases without named entities and cluster them with \n",
    "K-Means. \n",
    "\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2521a9",
   "metadata": {},
   "source": [
    "## Step 4: Get narrative statements based on the narrative model.\n",
    "\n",
    "----------------------------\n",
    "\n",
    "Once the narrative model is built, we can use it to extract narrative statements from any corpus (provided that the \n",
    "corpus is split into sentences and annotated for semantic roles). \n",
    "\n",
    "We call the function `get_narratives` for this purpose.\n",
    "\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6dc59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from relatio.wrappers import get_narratives\n",
    "\n",
    "final_statements = get_narratives(\n",
    "    srl_res=srl_res,\n",
    "    doc_index=split_sentences[0],  # doc names\n",
    "    narrative_model=narrative_model,\n",
    "    n_clusters=[0],  \n",
    "    progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc5303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The resulting pandas dataframe\n",
    "\n",
    "print(final_statements.columns)\n",
    "\n",
    "final_statements.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48de4356",
   "metadata": {},
   "source": [
    "### Trying out different clustering scenarios\n",
    "\n",
    "The choice of the number of clusters is corpus and application specific. Specifying a small number of latent entities leads to a large dimension reduction and may decrease entity coherence, whilst specifying a large number of latent entities may produce cluster redundancy. \n",
    "\n",
    "To help users try different clustering scenarios, the wrapper functions `build_narrative_model` and `get_narratives` allow users to experiment various clustering scenarios, which we detail below.\n",
    "\n",
    "----------------------------\n",
    "\n",
    "In `build_narrative_model`, the arguments `roles_with_embeddings` and `n_clusters` are specified as lists of lists arguments. This implies that you can cluster semantic roles separately (or together) and with different numbers of clusters. \n",
    "\n",
    "For example, a user could specify:\n",
    "- `roles_with_embeddings = [[\"ARG0\"],[\"ARG1\"]]`\n",
    "- `n_clusters = [[10,20],[10]]`\n",
    "    \n",
    "He/she would then cluster \"ARG0\" and \"ARG1\" separately and not consider \"ARG2\" for dimension reduction. For \"ARG0\", the clustering scenarios are a model with 10 and a model with 20 clusters. For \"ARG1\", the clustering scenario is a model with 10 clusters.\n",
    "\n",
    "\n",
    "----------------------------\n",
    "\n",
    "To extract narrative statements based on a clustering scenario, the `get_narratives` function also has the \n",
    "`n_clusters` argument. \n",
    "\n",
    "`n_clusters` is a list for the clustering scenarios. For instance, in our previous example, \n",
    "semantic roles \"ARG0\" and \"ARG1\" are clustered separately, so `n_clusters` expects two indices: one for the \n",
    "clustering scenario to pick for \"ARG0\" and one for the clustering scenario to pick for \"ARG1\".\n",
    "\n",
    "For example, a user could specify:\n",
    "- `n_clusters = [0,0]`\n",
    "\n",
    "He/she would then extract narrative statements based on 10 clusters for \"ARG0\" and 10 clusters for \"ARG1\".\n",
    "\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b238f719",
   "metadata": {},
   "source": [
    "## Step 5: Model validation and basic analysis\n",
    "\n",
    "----------------------------\n",
    "\n",
    "The resulting `final_statments` object is a pandas dataframe which lists narrative statements found in documents and \n",
    "sentences of the corpus.\n",
    "\n",
    "It is straight-forward to manually inspect the quality of the resulting entities and narrative statements.\n",
    "\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e554840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity coherence\n",
    "# Print most frequent phrases per entity\n",
    "\n",
    "# Pool ARG0, ARG1 and ARG2 together\n",
    "\n",
    "df1 = final_statements[['ARG0_lowdim', 'ARG0_highdim']]\n",
    "df1.rename(columns={'ARG0_lowdim': 'ARG', 'ARG0_highdim': 'ARG-RAW'}, inplace=True)\n",
    "\n",
    "df2 = final_statements[['ARG1_lowdim', 'ARG1_highdim']]\n",
    "df2.rename(columns={'ARG1_lowdim': 'ARG', 'ARG1_highdim': 'ARG-RAW'}, inplace=True)\n",
    "\n",
    "df3 = final_statements[['ARG2_lowdim', 'ARG2_highdim']]\n",
    "df3.rename(columns={'ARG2_lowdim': 'ARG', 'ARG2_highdim': 'ARG-RAW'}, inplace=True)\n",
    "\n",
    "df = df1.append(df2).reset_index(drop = True)\n",
    "df = df.append(df3).reset_index(drop = True)\n",
    "\n",
    "# Count semantic phrases\n",
    "\n",
    "df = df.groupby(['ARG', 'ARG-RAW']).size().reset_index()\n",
    "df.columns = ['ARG', 'ARG-RAW', 'count']\n",
    "\n",
    "# Drop empty semantic phrases\n",
    "\n",
    "df = df[df['ARG'] != ''] \n",
    "\n",
    "# Rearrange the data\n",
    "\n",
    "df = df.groupby(['ARG']).apply(lambda x: x.sort_values([\"count\"], ascending = False))\n",
    "df = df.reset_index(drop= True)\n",
    "df = df.groupby(['ARG']).head(10)\n",
    "\n",
    "df['ARG-RAW'] = df['ARG-RAW'] + ' - ' + df['count'].astype(str)\n",
    "df['cluster_elements'] = df.groupby(['ARG'])['ARG-RAW'].transform(lambda x: ' | '.join(x))\n",
    "\n",
    "df = df.drop_duplicates(subset=['ARG'])\n",
    "\n",
    "df['cluster_elements'] = [', '.join(set(i.split(','))) for i in list(df['cluster_elements'])]\n",
    "\n",
    "print('Entities to inspect:', len(df))\n",
    "\n",
    "df = df[['ARG', 'cluster_elements']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48de6015",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for l in df.values.tolist():\n",
    "    print('entity: \\n %s \\n' % l[0])\n",
    "    print('most frequent phrases: \\n %s \\n' % l[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6800a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low-dimensional vs. high-dimensional narrative statements\n",
    "\n",
    "# Replace negated verbs by \"not-verb\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "final_statements['B-V_lowdim_with_neg'] = np.where(final_statements['B-ARGM-NEG_lowdim'] == True, \n",
    "                                          'not-' + final_statements['B-V_lowdim'], \n",
    "                                          final_statements['B-V_lowdim'])\n",
    "\n",
    "final_statements['B-V_highdim_with_neg'] = np.where(final_statements['B-ARGM-NEG_highdim'] == True, \n",
    "                                           'not-' + final_statements['B-V_lowdim'], \n",
    "                                           final_statements['B-V_highdim'])\n",
    "\n",
    "# Concatenate high-dimensional narratives (with text preprocessing but no clustering)\n",
    "\n",
    "final_statements['narrative_highdim'] = (final_statements['ARG0_highdim'] + ' ' + \n",
    "                                         final_statements['B-V_highdim_with_neg'] + ' ' +  \n",
    "                                         final_statements['ARG1_highdim'])\n",
    "\n",
    "# Concatenate low-dimensional narratives (with clustering)\n",
    "\n",
    "final_statements['narrative_lowdim'] = (final_statements['ARG0_lowdim'] + ' ' + \n",
    "                                        final_statements['B-V_highdim_with_neg'] + ' ' + \n",
    "                                        final_statements['ARG1_lowdim'])\n",
    "\n",
    "# Focus on narratives with a ARG0-VERB-ARG1 structure (i.e. \"complete narratives\")\n",
    "\n",
    "indexNames = final_statements[(final_statements['ARG0_lowdim'] == '')|\n",
    "                             (final_statements['ARG1_lowdim'] == '')|\n",
    "                             (final_statements['B-V_lowdim_with_neg'] == '')].index\n",
    "\n",
    "complete_narratives = final_statements.drop(indexNames)\n",
    "\n",
    "complete_narratives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2ad32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top high-dimensional complete narrative statements\n",
    "\n",
    "complete_narratives['narrative_highdim'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1957c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top low-dimensional complete narrative statements\n",
    "\n",
    "complete_narratives['narrative_lowdim'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfdf95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print ten random complete narratives with and without dimension reduction\n",
    "#\n",
    "# Specifying a small number of clusters leads to a large dimension reduction \n",
    "# and may decrease cluster coherence, whilst specifying a large number of clusters\n",
    "# may produce cluster redundancy. \n",
    "#\n",
    "# The choice of the number of clusters is corpus and application specific \n",
    "# (and was chosen at random in this notebook).\n",
    "\n",
    "sample = complete_narratives.sample(10, random_state = 123).to_dict('records')\n",
    "\n",
    "for d in sample:\n",
    "    print('Original raw tweet: \\n %s \\n' %split_sentences[1][d['sentence']])\n",
    "    print('High-dimensional narrative: \\n %s \\n' %d['narrative_highdim'])\n",
    "    print('Low-dimensional narrative: \\n %s \\n' %d['narrative_lowdim'])\n",
    "    print('--------------------------------------------------- \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271ed864",
   "metadata": {},
   "source": [
    "## Step 6: Visualization // Plotting narrative graphs\n",
    "----------------------------\n",
    "\n",
    "A collection of narrative statements has an intuitive network structure, in which the edges are verbs and the nodes are entities.\n",
    "\n",
    "Here, we plot Trump's narrative statements on Twitter.\n",
    "\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319b2909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot low-dimensional complete narrative statements in a directed multi-graph\n",
    "\n",
    "from relatio.graphs import build_graph, draw_graph\n",
    "\n",
    "temp = complete_narratives[[\"ARG0_lowdim\", \"ARG1_lowdim\", \"B-V_lowdim\"]]\n",
    "temp.columns = [\"ARG0\", \"ARG1\", \"B-V\"]\n",
    "temp = temp[(temp[\"ARG0\"] != \"\") & (temp[\"ARG1\"] != \"\") & (temp[\"B-V\"] != \"\")]\n",
    "temp = temp.groupby([\"ARG0\", \"ARG1\", \"B-V\"]).size().reset_index(name=\"weight\")\n",
    "temp = temp.sort_values(by=\"weight\", ascending=False).iloc[\n",
    "    0:100\n",
    "]  # pick top 100 most frequent narratives\n",
    "temp = temp.to_dict(orient=\"records\")\n",
    "\n",
    "for l in temp:\n",
    "    l[\"color\"] = None\n",
    "\n",
    "G = build_graph(\n",
    "    dict_edges=temp, dict_args={}, edge_size=None, node_size=10, prune_network=True\n",
    ")\n",
    "\n",
    "draw_graph(G, notebook=True, output_filename=\"trump_worldview.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a final comment, note that you can look up the specifics of any function with the help command.\n",
    "\n",
    "help(build_narrative_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd177fa",
   "metadata": {},
   "source": [
    "## Miscellaneous\n",
    "\n",
    "----------------------------\n",
    "\n",
    "For a detailed discussion of semantic role annotations, please refer to: [Speech and Language Processing. Daniel Jurafsky & James H. Martin (2019)](https://web.stanford.edu/~jurafsky/slp3/old_oct19/20.pdf)\n",
    "\n",
    "----------------------------\n",
    "\n",
    "The package is still under development. We welcome comments, suggestions and bug reports:\n",
    "[File an issue](https://github.com/relatio-nlp/relatio/issues)\n",
    "\n",
    "----------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
