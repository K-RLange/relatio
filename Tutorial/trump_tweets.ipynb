{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "attempted-analyst",
   "metadata": {},
   "source": [
    "### Settings and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "nominated-payroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import numpy as np \n",
    "from numpy.linalg import norm\n",
    "\n",
    "from typing import Dict, List, NamedTuple, Optional, Tuple, Union, Any\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "import warnings\n",
    "import torch\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle as pk\n",
    "import json\n",
    "import time\n",
    "\n",
    "import re\n",
    "import string\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-opening",
   "metadata": {},
   "source": [
    "### All functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "differential-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "#..................................................................................................................\n",
    "#..................................................................................................................\n",
    "\n",
    "\n",
    "def split_into_sentences(\n",
    "    dataframe\n",
    "):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    A function that splits a list of documents into sentences (using the SpaCy sentence splitter).\n",
    "    \n",
    "    Args:\n",
    "        dataframe: a pandas dataframe with one column \"id\" and one column \"text\"\n",
    "        \n",
    "    Returns:\n",
    "        List of document ids and list of sentences\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    print('Splitting into sentences...')\n",
    "    \n",
    "    list_of_docs = dataframe.to_dict(orient = 'records')\n",
    "    \n",
    "    sentences = []\n",
    "    doc_indices = []\n",
    "    \n",
    "    for doc_info in list_of_docs:\n",
    "        for sent in nlp(doc_info['doc']).sents:\n",
    "            sent = str(sent)\n",
    "            sentences = sentences + [sent]\n",
    "            doc_indices = doc_indices + [doc_info['id']]\n",
    "    \n",
    "    return doc_indices, sentences\n",
    "\n",
    "\n",
    "def remove_extra_whitespaces(\n",
    "    s: str\n",
    ") -> str:\n",
    "    \n",
    "    res = \" \".join(s.split())\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "def _get_wordnet_pos(word):\n",
    "    \"\"\"Get POS tag\"\"\"\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "\n",
    "    return tag\n",
    "\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "f_lemmatize = wnl.lemmatize\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "    sentences: List[str],\n",
    "    remove_punctuation: bool = True,\n",
    "    remove_digits: bool = True,\n",
    "    remove_chars: str = \"\",\n",
    "    stop_words: Optional[List[str]] = None,\n",
    "    lowercase: bool = True,\n",
    "    strip: bool = True,\n",
    "    remove_whitespaces: bool = True,\n",
    "    lemmatize: bool = False,\n",
    "    stem: bool = False,\n",
    "    tags_to_keep: Optional[List[str]] = None,\n",
    "    remove_n_letter_words: Optional[int] = None,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Preprocess a list of sentences for word embedding.\n",
    "    Args:\n",
    "        sentence: list of sentences\n",
    "        remove_punctuation: whether to remove string.punctuation\n",
    "        remove_digits: whether to remove string.digits\n",
    "        remove_chars: remove the given characters\n",
    "        stop_words: list of stopwords to remove\n",
    "        lowercase: whether to lower the case\n",
    "        strip: whether to strip\n",
    "        remove_whitespaces: whether to remove superfluous whitespaceing by \" \".join(str.split(())\n",
    "        lemmatize: whether to lemmatize using nltk.WordNetLemmatizer\n",
    "        stem: whether to stem using nltk.SnowballStemmer(\"english\")\n",
    "        tags_to_keep: list of grammatical tags to keep (common tags: ['V', 'N', 'J'])\n",
    "        remove_n_letter_words: drop words lesser or equal to n letters (default is None)\n",
    "    Returns:\n",
    "        Processed list of sentences\n",
    "    Examples:\n",
    "        >>> preprocess([' Return the factorial of n, an  exact integer >= 0.'])\n",
    "        ['return the factorial of n an exact integer']\n",
    "        >>> preprocess(['Learning is usefull.'])\n",
    "        ['learning is usefull']\n",
    "        >>> preprocess([' Return the factorial of n, an  exact integer >= 0.'], stop_words=['factorial'])\n",
    "        ['return the of n an exact integer']\n",
    "        >>> preprocess([' Return the factorial of n, an  exact integer >= 0.'], lemmatize=True)\n",
    "        ['return the factorial of n an exact integer']\n",
    "        >>> preprocess(['Learning is usefull.'],lemmatize=True)\n",
    "        ['learn be usefull']\n",
    "        >>> preprocess([' Return the factorial of n, an  exact integer >= 0.'], stem=True)\n",
    "        ['return the factori of n an exact integ']\n",
    "        >>> preprocess(['Learning is usefull.'],stem=True)\n",
    "        ['learn is useful']\n",
    "        >>> preprocess(['A1b c\\\\n\\\\nde \\\\t fg\\\\rkl\\\\r\\\\n m+n'])\n",
    "        ['ab c de fg kl mn']\n",
    "        >>> preprocess(['This is a sentence with verbs and nice adjectives.'], tags_to_keep = ['V', 'J'])\n",
    "        ['is nice']\n",
    "        >>> preprocess(['This is a sentence with one and two letter words.'], remove_n_letter_words = 2)\n",
    "        ['this sentence with one and two letter words']\n",
    "    \"\"\"\n",
    "    if lemmatize is True and stem is True:\n",
    "        raise ValueError(\"lemmatize and stemming cannot be both True\")\n",
    "    if stop_words is not None and lowercase is False:\n",
    "        raise ValueError(\"remove stop words make sense only for lowercase\")\n",
    "\n",
    "    # remove chars\n",
    "    if remove_punctuation:\n",
    "        remove_chars += string.punctuation\n",
    "    if remove_digits:\n",
    "        remove_chars += string.digits\n",
    "    if remove_chars:\n",
    "        sentences = [re.sub(f\"[{remove_chars}]\", \"\", str(sent)) for sent in sentences]\n",
    "\n",
    "    # lowercase, strip and remove superfluous white spaces\n",
    "    if lowercase:\n",
    "        sentences = [sent.lower() for sent in sentences]\n",
    "    if strip:\n",
    "        sentences = [sent.strip() for sent in sentences]\n",
    "    if remove_whitespaces:\n",
    "        sentences = [\" \".join(sent.split()) for sent in sentences]\n",
    "\n",
    "    # lemmatize\n",
    "    if lemmatize:\n",
    "\n",
    "        tag_dict = {\n",
    "            \"J\": wordnet.ADJ,\n",
    "            \"N\": wordnet.NOUN,\n",
    "            \"V\": wordnet.VERB,\n",
    "            \"R\": wordnet.ADV,\n",
    "        }\n",
    "\n",
    "        sentences = [\n",
    "            \" \".join(\n",
    "                [\n",
    "                    f_lemmatize(\n",
    "                        word, tag_dict.get(_get_wordnet_pos(word), wordnet.NOUN)\n",
    "                    )\n",
    "                    for word in sent.split()\n",
    "                ]\n",
    "            )\n",
    "            for sent in sentences\n",
    "        ]\n",
    "\n",
    "    # keep specific nltk tags\n",
    "    # this step should be performed before stemming, but may be performed after lemmatization\n",
    "    if tags_to_keep is not None:\n",
    "        sentences = [\n",
    "            \" \".join(\n",
    "                [\n",
    "                    word\n",
    "                    for word in sent.split()\n",
    "                    if _get_wordnet_pos(word) in tags_to_keep\n",
    "                ]\n",
    "            )\n",
    "            for sent in sentences\n",
    "        ]\n",
    "\n",
    "    # stem\n",
    "    if stem:\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        f_stem = stemmer.stem\n",
    "\n",
    "        sentences = [\n",
    "            \" \".join([f_stem(word) for word in sent.split()]) for sent in sentences\n",
    "        ]\n",
    "\n",
    "    # drop stopwords\n",
    "    # stopwords are dropped after the bulk of preprocessing steps, so they should also be preprocessed with the same standards\n",
    "    if stop_words is not None:\n",
    "        sentences = [\n",
    "            \" \".join([word for word in sent.split() if word not in stop_words])\n",
    "            for sent in sentences\n",
    "        ]\n",
    "\n",
    "    # remove short words < n\n",
    "    if remove_n_letter_words is not None:\n",
    "        sentences = [\n",
    "            \" \".join(\n",
    "                [word for word in sent.split() if len(word) > remove_n_letter_words]\n",
    "            )\n",
    "            for sent in sentences\n",
    "        ]\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def get_role_counts(\n",
    "    statements: List[dict],\n",
    "    roles: Optional[list] = [\"B-V\", \"ARGO\", \"ARG1\", \"ARG2\"],\n",
    ") -> dict:\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Get role frequency within the corpus from preprocessed semantic roles. Roles considered are specified by the user.\n",
    "    Args:\n",
    "        statements: list of dictionaries of postprocessed semantic roles\n",
    "        roles: list of roles considered\n",
    "    Returns:\n",
    "        Dictionary in which postprocessed semantic roles are keys and their frequency within the corpus are values\n",
    "        (e.g. d['verb'] = count)\n",
    "        \n",
    "    Example:\n",
    "        >>> test = [{'B-V': ['increase'], 'B-ARGM-NEG': True},{'B-V': ['decrease']},{'B-V': ['decrease']}]\\n\n",
    "        ... verb_counts = get_role_counts(test, roles = ['B-V'])\n",
    "        {'increase': 1, 'decrease': 2}\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    counts = {}\n",
    "\n",
    "    for statement in statements:\n",
    "        for key in statement.keys():\n",
    "            if key in roles:\n",
    "                temp = \" \".join(statement[key])\n",
    "                if temp in counts:\n",
    "                    counts[temp] += 1\n",
    "                else:\n",
    "                    counts[temp] = 1\n",
    "\n",
    "    return counts\n",
    "\n",
    "\n",
    "# Semantic Role Labeling\n",
    "#..................................................................................................................\n",
    "#..................................................................................................................\n",
    "\n",
    "# link to choose the SRL model \n",
    "# https://storage.googleapis.com/allennlp-public-models/YOUR-PREFERRED-MODEL\n",
    "\n",
    "\n",
    "def filter_sentences(\n",
    "    sentences: List[str],\n",
    "    max_sentence_length: Optional[int] = None,\n",
    "    max_number_words: Optional[int] = None,\n",
    ") -> List[str]:\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Filter list of sentences based on the number of characters length.\n",
    "    Args:\n",
    "        max_sentence_length: Keep only sentences with a a number of character lower or equal to max_sentence_length. For max_number_words = max_sentence_length = -1 all sentences are kept.\n",
    "        max_number_words: Keep only sentences with a a number of words lower or equal to max_number_words. For max_number_words = max_sentence_length = -1 all sentences are kept.\n",
    "    Returns:\n",
    "        Filtered list of sentences.\n",
    "    Examples:\n",
    "        >>> filter_sentences(['This is a house'])\n",
    "        ['This is a house']\n",
    "        >>> filter_sentences(['This is a house'], max_sentence_length=15)\n",
    "        ['This is a house']\n",
    "        >>> filter_sentences(['This is a house'], max_sentence_length=14)\n",
    "        []\n",
    "        >>> filter_sentences(['This is a house'], max_number_words=4)\n",
    "        ['This is a house']\n",
    "        >>> filter_sentences(['This is a house'], max_number_words=3)\n",
    "        []\n",
    "        >>> filter_sentences(['This is a house', 'It is a nice house'], max_number_words=5, max_sentence_length=18)\n",
    "        ['This is a house', 'It is a nice house']\n",
    "        >>> filter_sentences(['This is a house', 'It is a nice house'], max_number_words=4, max_sentence_length=18)\n",
    "        ['This is a house']\n",
    "        >>> filter_sentences(['This is a house', 'It is a nice house'], max_number_words=5, max_sentence_length=17)\n",
    "        ['This is a house']\n",
    "        >>> filter_sentences(['This is a house', 'It is a nice house'], max_number_words=0, max_sentence_length=18)\n",
    "        []\n",
    "        >>> filter_sentences(['This is a house', 'It is a nice house'], max_number_words=5, max_sentence_length=0)\n",
    "        []\n",
    "        >>> filter_sentences(['This is a house', 'It is a nice house'])\n",
    "        ['This is a house', 'It is a nice house']\n",
    "        >>> filter_sentences(['This is a house', 'It is a nice house'], max_number_words=4)\n",
    "        ['This is a house']\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    if max_sentence_length is None and max_number_words is None:\n",
    "        pass\n",
    "    elif max_sentence_length == 0 or max_number_words == 0:\n",
    "        sentences = []\n",
    "    else:\n",
    "        if max_sentence_length is not None:\n",
    "            sentences = [sent for sent in sentences if len(sent) <= max_sentence_length]\n",
    "\n",
    "            def filter_funct(sent):\n",
    "                return len(sent) <= max_sentence_length\n",
    "\n",
    "        if max_number_words is not None:\n",
    "            sentences = [\n",
    "                sent for sent in sentences if len(sent.split()) <= max_number_words\n",
    "            ]\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def group_sentences_in_batches(\n",
    "    sentences: List[str],\n",
    "    max_batch_char_length: Optional[int] = None,\n",
    "    batch_size: Optional[int] = None,\n",
    ") -> List[List[str]]:\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Group sentences in batches of given total character length.\n",
    "    Args:\n",
    "        sentences: List of sentences\n",
    "        max_batch_char_length: maximum char length for a batch\n",
    "    Returns:\n",
    "        List of batches (list) of sentences.\n",
    "    Examples:\n",
    "        >>> group_sentences_in_batches(['This is a house','This is a house'], max_batch_char_length=15)\n",
    "        [['This is a house'], ['This is a house']]\n",
    "        >>> group_sentences_in_batches(['This is a house','This is a house'], max_batch_char_length=14)\n",
    "        []\n",
    "        >>> group_sentences_in_batches(['This is a house','This is a house'], max_batch_char_length=29)\n",
    "        [['This is a house'], ['This is a house']]\n",
    "        >>> group_sentences_in_batches(['This is a house','This is a house'], max_batch_char_length=30)\n",
    "        [['This is a house', 'This is a house']]\n",
    "        >>> group_sentences_in_batches(['This is a house','This is a house'])\n",
    "        [['This is a house', 'This is a house']]\n",
    "        >>> group_sentences_in_batches(['This is a house','This is a house','This is a house'], max_batch_char_length=29)\n",
    "        [['This is a house'], ['This is a house'], ['This is a house']]\n",
    "        >>> group_sentences_in_batches(['This is a house','This is a house','This is a house'], batch_size=2)\n",
    "        [['This is a house', 'This is a house'], ['This is a house']]\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    batches: List[List[str]] = []\n",
    "\n",
    "    if max_batch_char_length is None and batch_size is None:\n",
    "        batches = [sentences]\n",
    "    elif max_batch_char_length is not None and batch_size is not None:\n",
    "        raise ValueError(\"max_batch_char_length and batch_size are mutual exclusive.\")\n",
    "    elif batch_size is not None:\n",
    "        batches = [\n",
    "            sentences[i : i + batch_size] for i in range(0, len(sentences), batch_size)\n",
    "        ]\n",
    "    else:\n",
    "        batch_char_length = 0\n",
    "        batch: List[str] = []\n",
    "\n",
    "        for el in sentences:\n",
    "            length = len(el)\n",
    "            batch_char_length += length\n",
    "            if length > max_batch_char_length:\n",
    "                warnings.warn(\n",
    "                    f\"The length of the sentence = {length} > max_batch_length={max_batch_char_length}. The following sentence is skipped: \\n > {el}\",\n",
    "                    RuntimeWarning,\n",
    "                )\n",
    "                continue\n",
    "            if batch_char_length > max_batch_char_length:\n",
    "                batches.append(batch)\n",
    "                batch = [el]\n",
    "                batch_char_length = length\n",
    "            else:\n",
    "                batch.append(el)\n",
    "\n",
    "        if batch:\n",
    "            batches.append(batch)\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "class SRL:\n",
    "    def __init__(\n",
    "        self,\n",
    "        path: str,\n",
    "        cuda_device: int = -1,\n",
    "        max_batch_char_length: Optional[int] = None,\n",
    "        batch_size: Optional[int] = None,\n",
    "        max_sentence_length: Optional[int] = None,\n",
    "        max_number_words: Optional[int] = None,\n",
    "        cuda_empty_cache: bool = True,\n",
    "        cuda_sleep: float = 0.0,\n",
    "    ):\n",
    "        self._predictor = Predictor.from_path(path, cuda_device=cuda_device)\n",
    "        self._max_batch_char_length = max_batch_char_length\n",
    "        self._batch_size = batch_size\n",
    "        self._max_sentence_length = max_sentence_length\n",
    "        self._max_number_words = max_number_words\n",
    "        self._cuda_empty_cache = cuda_empty_cache\n",
    "        self._cuda_device = cuda_device\n",
    "        self._cuda_sleep = cuda_sleep\n",
    "\n",
    "    def _clean_cache(self, cuda_sleep, cuda_empty_cache):\n",
    "        if self._cuda_device > -1 and cuda_empty_cache:\n",
    "            with torch.cuda.device(self._cuda_device):\n",
    "                torch.cuda.empty_cache()\n",
    "                time.sleep(cuda_sleep)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        sentences: List[str],\n",
    "        max_batch_char_length: Optional[int] = None,\n",
    "        batch_size: Optional[int] = None,\n",
    "        max_sentence_length: Optional[int] = None,\n",
    "        max_number_words: Optional[int] = None,\n",
    "        cuda_empty_cache: bool = None,\n",
    "        cuda_sleep: float = None,\n",
    "    ):\n",
    "        max_batch_char_length = (\n",
    "            max_batch_char_length\n",
    "            if max_batch_char_length is not None\n",
    "            else self._max_batch_char_length\n",
    "        )\n",
    "\n",
    "        batch_size = batch_size if batch_size is not None else self._batch_size\n",
    "\n",
    "        max_sentence_length = (\n",
    "            max_sentence_length\n",
    "            if max_sentence_length is not None\n",
    "            else self._max_sentence_length\n",
    "        )\n",
    "\n",
    "        max_number_words = (\n",
    "            max_number_words if max_number_words is not None else self._max_number_words\n",
    "        )\n",
    "\n",
    "        cuda_empty_cache = (\n",
    "            cuda_empty_cache if cuda_empty_cache is not None else self._cuda_empty_cache\n",
    "        )\n",
    "\n",
    "        cuda_sleep = cuda_sleep if cuda_sleep is not None else self._cuda_sleep\n",
    "\n",
    "        sentences = filter_sentences(\n",
    "            sentences,\n",
    "            max_sentence_length=max_sentence_length,\n",
    "            max_number_words=max_number_words,\n",
    "        )\n",
    "\n",
    "        batches = group_sentences_in_batches(\n",
    "            sentences,\n",
    "            max_batch_char_length=max_batch_char_length,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "\n",
    "        res = []\n",
    "        for batch in tqdm(batches):\n",
    "            sentences_json = [{\"sentence\": sent} for sent in batch]\n",
    "            try:\n",
    "                res_batch = self._predictor.predict_batch_json(sentences_json)\n",
    "            except RuntimeError as err:\n",
    "                warnings.warn(\n",
    "                    f\"empty result {err}\",\n",
    "                    RuntimeWarning,\n",
    "                )\n",
    "                res = [None]\n",
    "                break\n",
    "            except:\n",
    "                raise\n",
    "            finally:\n",
    "                self._clean_cache(cuda_sleep, cuda_empty_cache)\n",
    "\n",
    "            res.extend(res_batch)\n",
    "        return res\n",
    "    \n",
    "    \n",
    "def extract_roles(\n",
    "    srl: List[Dict[str, Any]],\n",
    "    UsedRoles: List[str]\n",
    ") -> Tuple[List[Dict[str, List]], List[int]]:\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    A function that extracts semantic roles from the SRL output.\n",
    "    \n",
    "    Args:\n",
    "        srl: srl output\n",
    "        UsedRoles: dict with the specifics of the pipeline for each role\n",
    "        \n",
    "    Returns:\n",
    "        List of statements and numpy array of sentence indices (to keep track of sentences)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    statements_role_list: List[Dict[str, List]] = []\n",
    "    sentence_index: List[int] = []\n",
    "        \n",
    "    for i, sentence_dict in enumerate(srl):\n",
    "        role_per_sentence = extract_role_per_sentence(sentence_dict, UsedRoles)\n",
    "        sentence_index.extend([i] * len(role_per_sentence))\n",
    "        statements_role_list.extend(role_per_sentence)\n",
    "\n",
    "    return statements_role_list, np.asarray(sentence_index, dtype=np.uint32)\n",
    "\n",
    "\n",
    "def extract_role_per_sentence(\n",
    "    sentence_dict: dict, \n",
    "    UsedRoles: List[str]\n",
    ") -> List[dict]:\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    A function that extracts the semantic roles for a given sentence.\n",
    "    \n",
    "    Args:\n",
    "        srl: srl output\n",
    "        UsedRoles: dict with the specifics of the pipeline for each role\n",
    "        \n",
    "    Returns:\n",
    "        List of statements with their associated roles for a given sentence\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    word_list = sentence_dict[\"words\"]\n",
    "    sentence_role_list = []\n",
    "    \n",
    "    for statement_dict in sentence_dict[\"verbs\"]:\n",
    "        tag_list = statement_dict[\"tags\"]\n",
    "\n",
    "        statement_role_dict = {}\n",
    "\n",
    "        if 'ARGO' in UsedRoles:\n",
    "            indices_agent = [i for i, tok in enumerate(tag_list) if \"ARG0\" in tok]\n",
    "            agent = [tok for i, tok in enumerate(word_list) if i in indices_agent]\n",
    "            statement_role_dict[\"ARGO\"] = agent\n",
    "        \n",
    "        if 'ARG1' in UsedRoles:\n",
    "            indices_patient = [i for i, tok in enumerate(tag_list) if \"ARG1\" in tok]\n",
    "            patient = [tok for i, tok in enumerate(word_list) if i in indices_patient]\n",
    "            statement_role_dict[\"ARG1\"] = patient\n",
    "        \n",
    "        if 'ARG2' in UsedRoles:\n",
    "            indices_attribute = [i for i, tok in enumerate(tag_list) if \"ARG2\" in tok]\n",
    "            attribute = [tok for i, tok in enumerate(word_list) if i in indices_attribute]\n",
    "            statement_role_dict[\"ARG2\"] = attribute\n",
    "        \n",
    "        if 'B-V' in UsedRoles:\n",
    "            indices_verb = [i for i, tok in enumerate(tag_list) if \"B-V\" in tok]\n",
    "            verb = [tok for i, tok in enumerate(word_list) if i in indices_verb]\n",
    "            statement_role_dict[\"B-V\"] = verb\n",
    "        \n",
    "        if 'B-ARGM-MOD' in UsedRoles:\n",
    "            indices_modal = [i for i, tok in enumerate(tag_list) if \"B-ARGM-MOD\" in tok]\n",
    "            modal = [tok for i, tok in enumerate(word_list) if i in indices_modal]\n",
    "            statement_role_dict[\"B-ARGM-MOD\"] = modal\n",
    "            \n",
    "        if 'B-ARGM-NEG' in UsedRoles:\n",
    "            role_negation_value = any(\"B-ARGM-NEG\" in tag for tag in tag_list)\n",
    "            statement_role_dict[\"B-ARGM-NEG\"] = role_negation_value\n",
    "\n",
    "        key_to_delete = []\n",
    "        for key, value in statement_role_dict.items():\n",
    "            if not value:\n",
    "                key_to_delete.append(key)\n",
    "        for key in key_to_delete:\n",
    "            del statement_role_dict[key]\n",
    "        sentence_role_list.append(statement_role_dict)\n",
    "            \n",
    "    if not sentence_role_list:\n",
    "        sentence_role_list = [{}]\n",
    "        \n",
    "    return sentence_role_list\n",
    "\n",
    "\n",
    "def postprocess_roles(\n",
    "    statements: List[Dict[str, List]],\n",
    "    max_length: Optional[int] = None,\n",
    "    remove_punctuation: bool = True,\n",
    "    remove_digits: bool = True,\n",
    "    remove_chars: str = \"\",\n",
    "    stop_words: Optional[List[str]] = None,\n",
    "    lowercase: bool = True,\n",
    "    strip: bool = True,\n",
    "    remove_whitespaces: bool = True,\n",
    "    lemmatize: bool = False,\n",
    "    stem: bool = False,\n",
    "    tags_to_keep: Optional[List[str]] = None,\n",
    "    remove_n_letter_words: Optional[int] = None,\n",
    ") -> List[Dict[str, List]]:\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    max_length = remove roles of more than n tokens (NB: very long roles tend to be uninformative in our context)\n",
    "    For other arguments see utils.preprocess .\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    roles_copy = deepcopy(statements)\n",
    "    for i, statement in enumerate(statements):\n",
    "        for role, tokens in statements[i].items():\n",
    "            if isinstance(tokens, list):\n",
    "                res = [\n",
    "                    preprocess(\n",
    "                        [\" \".join(tokens)],\n",
    "                        remove_punctuation=remove_punctuation,\n",
    "                        remove_digits=remove_digits,\n",
    "                        remove_chars=remove_chars,\n",
    "                        stop_words=stop_words,\n",
    "                        lowercase=lowercase,\n",
    "                        strip=strip,\n",
    "                        remove_whitespaces=remove_whitespaces,\n",
    "                        lemmatize=lemmatize,\n",
    "                        stem=stem,\n",
    "                        tags_to_keep=tags_to_keep,\n",
    "                        remove_n_letter_words=remove_n_letter_words,\n",
    "                    )[0].split()\n",
    "                ][0]\n",
    "                if max_length is not None:\n",
    "                    if len(res) <= max_length:\n",
    "                        roles_copy[i][role] = res\n",
    "                    else:\n",
    "                        roles_copy[i][role] = []\n",
    "                else:\n",
    "                    roles_copy[i][role] = res\n",
    "            elif isinstance(tokens, bool):\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(f\"{tokens}\")\n",
    "    return roles_copy\n",
    "\n",
    "\n",
    "# Named Entity Recognition\n",
    "#..................................................................................................................\n",
    "#..................................................................................................................\n",
    "\n",
    "\n",
    "def mine_entities(\n",
    "    sentences: List[str],\n",
    "    ent_labels: Optional[List[str]] = ['PERSON', 'NORP', 'ORG', 'GPE', 'EVENT'],\n",
    "    remove_punctuation: bool = True,\n",
    "    remove_digits: bool = True,\n",
    "    remove_chars: str = \"\",\n",
    "    stop_words: Optional[List[str]] = None,\n",
    "    lowercase: bool = True,\n",
    "    strip: bool = True,\n",
    "    remove_whitespaces: bool = True,\n",
    "    lemmatize: bool = False,\n",
    "    stem: bool = False,\n",
    "    tags_to_keep: Optional[List[str]] = None,\n",
    "    remove_n_letter_words: Optional[int] = None\n",
    ") -> List[Tuple[str, int]]:\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    A function that goes through sentences and counts named entities found in the corpus.\n",
    "    \n",
    "    Args:\n",
    "        sentences: list of sentences\n",
    "        ent_labels: list of entity labels to be considered (see SPaCy documentation)\n",
    "        \n",
    "    Returns:\n",
    "        List of tuples with the named entity and its associated frequency on the corpus\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    entities_all = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = nlp(sentence)\n",
    "        for ent in sentence.ents:\n",
    "            if ent.label_ in ent_labels:\n",
    "                entity = [ent.text]\n",
    "                entities_all = entity + entities_all\n",
    "\n",
    "    entities_all = preprocess(entities_all,\n",
    "                              remove_punctuation,\n",
    "                              remove_digits,\n",
    "                              remove_chars,\n",
    "                              stop_words,\n",
    "                              lowercase,\n",
    "                              strip,\n",
    "                              remove_whitespaces,\n",
    "                              lemmatize,\n",
    "                              stem,\n",
    "                              tags_to_keep,\n",
    "                              remove_n_letter_words) \n",
    "\n",
    "    entity_counts = Counter(entities_all)\n",
    "    entities_sorted = sorted(entity_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return entities_sorted\n",
    "\n",
    "\n",
    "def pick_top_entities(\n",
    "    entities_sorted: List[Tuple[str,int]],\n",
    "    top_n_entities: Optional[int] = 0\n",
    ") -> List[str]:\n",
    "   \n",
    "    \"\"\"\n",
    "    \n",
    "    A function that returns the top n most frequent named entities in the corpus.\n",
    "    \n",
    "    Args:\n",
    "        entities_sorted: list of tuples (named_entity, frequency)\n",
    "        top_n_entities: number of named entities to keep (default is all and is specified with top_n = 0)\n",
    "        \n",
    "    Returns:\n",
    "        List of most frequent named entities\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    entities = []\n",
    "\n",
    "    for entity in entities_sorted:\n",
    "        entities = entities + [entity[0]]\n",
    "    \n",
    "    if top_n_entities == 0:\n",
    "        top_n_entities = len(entities_sorted)\n",
    "    \n",
    "    return entities[0:top_n_entities]\n",
    "\n",
    "\n",
    "def is_subsequence(\n",
    "    v2: list, \n",
    "    v1: list\n",
    ") -> bool:\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Check whether v2 is a subsequence of v1.\n",
    "    \n",
    "    Args:\n",
    "        v2/v1: lists of elements\n",
    "        \n",
    "    Returns:\n",
    "        a boolean\n",
    "    \n",
    "    Example:\n",
    "        >>> v1 = ['the', 'united', 'states', 'of', 'america']\\n\n",
    "        ... v2 = ['united', 'states', 'of', 'europe']\\n\n",
    "        ... is_subsequence(v2,v1)\n",
    "        False\n",
    "    \n",
    "    \"\"\"\n",
    "    it = iter(v1)\n",
    "    return all(c in it for c in v2) \n",
    "\n",
    "\n",
    "def map_entities(\n",
    "    statements: List[dict],\n",
    "    entities: list,\n",
    "    UsedRoles: List[str]\n",
    ") -> Tuple[dict, List[dict]]:\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    A function that goes through statements and identifies pre-defined named entities within postprocessed semantic roles.\n",
    "    \n",
    "    Args:\n",
    "        statements: list of dictionaries of postprocessed semantic roles\n",
    "        entities: user-defined list of named entities \n",
    "        roles: a list of roles with named entities (default = ARG0 and ARG1)\n",
    "        UsedRoles: list of roles for named entity recognition\n",
    "        \n",
    "    Returns:\n",
    "        entity_index: dictionary containing statements indices with entities for each role\n",
    "        roles_copy: new list of postprocessed semantic roles (without the named entities mined since they will not be embedded)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    entity_index = {role:{entity:np.asarray([], dtype=int) for entity in entities} for role in UsedRoles}\n",
    "    \n",
    "    roles_copy = deepcopy(statements)\n",
    "    \n",
    "    for i, statement in enumerate(statements):\n",
    "        for role, tokens in statements[i].items():\n",
    "            if role in UsedRoles:\n",
    "                for entity in entities:\n",
    "                    if is_subsequence(entity.split(), tokens)  == True: \n",
    "                        entity_index[role][entity] = np.append(entity_index[role][entity], [i]) \n",
    "                        roles_copy[i][role] = []\n",
    "    \n",
    "    return entity_index, roles_copy\n",
    "\n",
    "\n",
    "# Clean Verbs\n",
    "#..................................................................................................................\n",
    "#..................................................................................................................\n",
    "\n",
    "\n",
    "def find_synonyms(verb: str) -> List[str]:\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Find synonyms of a given word based on wordnet.\n",
    "    Args:\n",
    "        verb: a verb\n",
    "    Returns:\n",
    "        a list of synonyms\n",
    "    Example:\n",
    "        >>> find_synonyms('fight')\n",
    "        ['contend', 'fight', 'struggle', 'fight', 'oppose', 'fight_back', 'fight_down', 'defend', 'fight', 'struggle', 'crusade', 'fight', 'press', 'campaign', 'push', 'agitate']\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    synonyms = []\n",
    "    \n",
    "    for syn in wordnet.synsets(verb, pos=wordnet.VERB):\n",
    "        for l in syn.lemmas():\n",
    "            synonyms.append(l.name())\n",
    "            \n",
    "    return synonyms\n",
    "\n",
    "\n",
    "def find_antonyms(verb: str) -> List[str]:\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Find antonyms of a given word based on wordnet.\n",
    "    Args:\n",
    "        verb: a verb\n",
    "    Returns:\n",
    "        a list of antonyms\n",
    "    Example:\n",
    "        >>> find_antonyms('break')\n",
    "        ['repair', 'keep', 'conform_to', 'make', 'promote']\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    antonyms = []\n",
    "    \n",
    "    for syn in wordnet.synsets(verb, pos=wordnet.VERB):\n",
    "        for l in syn.lemmas():\n",
    "            if l.antonyms():\n",
    "                antonyms.append(l.antonyms()[0].name())\n",
    "                \n",
    "    return antonyms\n",
    "\n",
    "\n",
    "def get_most_frequent(tokens: List[str], token_counts: dict) -> str:\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Find most frequent token in a list of tokens. \n",
    "    \n",
    "    Args:\n",
    "        tokens: a list of tokens\n",
    "        token_counts: a dictionary of token frequencies\n",
    "    Returns:\n",
    "        the most frequent token in the list of tokens\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    freq = 0\n",
    "    most_freq_token = None\n",
    "    \n",
    "    for candidate in tokens:\n",
    "        if candidate in token_counts:\n",
    "            if token_counts[candidate] > freq:\n",
    "                freq = token_counts[candidate]\n",
    "                most_freq_token = candidate        \n",
    "                \n",
    "    return most_freq_token\n",
    "    \n",
    "\n",
    "def clean_verbs(statements: List[dict], verb_counts: dict) -> List[dict]:\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Replace verbs by their most frequent synonym or antonym.\n",
    "    If a verb is combined with a negation in the statement (e.g. 'not increase'),\n",
    "    it is replaced by its most frequent antonym and the negation is removed (e.g. \"decrease\").\n",
    "    Args:\n",
    "        statements: a list of dictionaries of postprocessed semantic roles\n",
    "        verb_counts: a dictionary of verb counts (e.g. d['verb'] = count)\n",
    "    Returns:\n",
    "        a list of dictionaries of postprocessed semantic roles with replaced verbs (same format as statements)\n",
    "        \n",
    "    Example:\n",
    "        >>> test = [{'B-V': ['increase'], 'B-ARGM-NEG': True},{'B-V': ['decrease']},{'B-V': ['decrease']}]\\n\n",
    "        ... verb_counts = get_role_counts(test, roles = ['B-V'])\\n\n",
    "        ... clean_verbs(test, verb_counts = verb_counts)\n",
    "        [{'B-V-CLEANED': 'decrease'}, {'B-V-CLEANED': 'decrease'}, {'B-V-CLEANED': 'decrease'}]\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    new_roles_all = []\n",
    "    \n",
    "    for roles in statements:\n",
    "        new_roles = deepcopy(roles)\n",
    "        new_roles = {str(k + '-CLEANED'): v for k, v in new_roles.items() if k in ['B-V', 'B-ARGM-NEG']}\n",
    "        if \"B-V\" in roles:\n",
    "            verb = \" \".join(new_roles[\"B-V-CLEANED\"])\n",
    "            new_roles[\"B-V-CLEANED\"] = verb\n",
    "            if \"B-ARGM-NEG\" in roles:\n",
    "                verbs = find_antonyms(verb) \n",
    "                most_freq_verb = get_most_frequent(tokens = verbs, token_counts = verb_counts)\n",
    "                if most_freq_verb is not None:\n",
    "                    new_roles[\"B-V-CLEANED\"] = most_freq_verb\n",
    "                    del new_roles[\"B-ARGM-NEG-CLEANED\"]\n",
    "            else:\n",
    "                verbs = find_synonyms(verb) + [verb]\n",
    "                most_freq_verb = get_most_frequent(tokens = verbs, token_counts = verb_counts)\n",
    "                if most_freq_verb is not None:\n",
    "                    new_roles[\"B-V-CLEANED\"] = most_freq_verb\n",
    "        new_roles_all.append(new_roles)\n",
    "    \n",
    "    return new_roles_all\n",
    "\n",
    "\n",
    "# Vectors and Clustering\n",
    "#..................................................................................................................\n",
    "#..................................................................................................................\n",
    "\n",
    "def count_words(\n",
    "    sentences: List[str]\n",
    ") -> dict:\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    A function that computes word frequencies in a list of sentences.\n",
    "    \n",
    "    Args:\n",
    "        sentences: list of sentences\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary {\"word\": frequency}\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    words = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = words + sentence.split()\n",
    "\n",
    "    word_count_dict = dict(Counter(words))\n",
    "    \n",
    "    return word_count_dict\n",
    "\n",
    "\n",
    "def compute_sif_weights(\n",
    "    word_count_dict: dict,\n",
    "    alpha: Optional[float] = 0.001\n",
    ") -> dict:\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    A function that computes SIF weights based on word frequencies.\n",
    "    \n",
    "    Args:\n",
    "        word_count_dict: a dictionary {\"word\": frequency}\n",
    "        alpha: regularization parameter (see original paper)\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary {\"word\": SIF weight}\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    sif_dict = {}\n",
    "    \n",
    "    for word, count in word_count_dict.items():\n",
    "        sif_dict[word] = alpha / (alpha + count)\n",
    "\n",
    "    return sif_dict\n",
    "\n",
    "\n",
    "class USE:\n",
    "    def __init__(self, path: str):\n",
    "        self._embed = hub.load(path)\n",
    "\n",
    "    def __call__(self, tokens: List[str]) -> np.ndarray:\n",
    "        return self._embed([\" \".join(tokens)]).numpy()[0]\n",
    "\n",
    "\n",
    "class SIF_word2vec:\n",
    "    def __init__(\n",
    "        self, path: str, sentences = List[str], alpha: Optional[float] = 0.001, normalize: bool = True\n",
    "    ):\n",
    "\n",
    "        self._model = Word2Vec.load(path)\n",
    "\n",
    "        self._word_count_dict = count_words(sentences)\n",
    "        \n",
    "        self._sif_dict = compute_sif_weights(self._word_count_dict, alpha)\n",
    "        \n",
    "        self._vocab = self._model.wv.vocab\n",
    "\n",
    "        self._normalize = normalize\n",
    "\n",
    "    def __call__(self, tokens: List[str]):\n",
    "        res = np.mean(\n",
    "            [self._sif_dict[token] * self._model.wv[token] for token in tokens], axis=0\n",
    "        )\n",
    "        if self._normalize:\n",
    "            res = res / norm(res)  \n",
    "        return res\n",
    "    \n",
    "\n",
    "class SIF_keyed_vectors:\n",
    "    def __init__(\n",
    "        self, path: str, sentences = List[str], alpha: Optional[float] = 0.001, normalize: bool = True\n",
    "    ):\n",
    "\n",
    "        self._model = api.load(path)\n",
    "\n",
    "        self._word_count_dict = count_words(sentences)\n",
    "        \n",
    "        self._sif_dict = compute_sif_weights(self._word_count_dict, alpha)\n",
    "        \n",
    "        self._vocab = self._model.vocab\n",
    "\n",
    "        self._normalize = normalize\n",
    "\n",
    "    def __call__(self, tokens: List[str]):\n",
    "        res = np.mean(\n",
    "            [self._sif_dict[token] * self._model[token] for token in tokens], axis=0\n",
    "        )\n",
    "        if self._normalize:\n",
    "            res = res / norm(res)  \n",
    "        return res\n",
    "\n",
    "\n",
    "def get_vector(\n",
    "    tokens: List[str],\n",
    "    model: Union[USE, SIF_word2vec, SIF_keyed_vectors]\n",
    "):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    A function that computes an embedding vector for a list of tokens.\n",
    "    \n",
    "    Args:\n",
    "        tokens: list of string tokens to embed\n",
    "        model: trained embedding model \n",
    "        (e.g. either Universal Sentence Encoders, a full gensim Word2Vec model or gensim Keyed Vectors)\n",
    "        \n",
    "    Returns:\n",
    "        A two-dimensional numpy array (1,dimension of the embedding space)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(model, (USE, SIF_word2vec, SIF_keyed_vectors)):\n",
    "        raise TypeError(\"Union[USE, SIF_Word2Vec, SIF_keyed_vectors]\")\n",
    "    \n",
    "    if isinstance(model, SIF_word2vec) or isinstance(model, SIF_keyed_vectors): \n",
    "        if not tokens:\n",
    "            res = None\n",
    "        elif any(token not in model._sif_dict for token in tokens):\n",
    "            res = None\n",
    "        elif any(token not in model._vocab for token in tokens): \n",
    "            res = None \n",
    "        else:\n",
    "            res = model(tokens)\n",
    "            res = np.array([res]) # correct format to feed the vectors to sklearn clustering methods\n",
    "    else:\n",
    "            res = model(tokens)\n",
    "            res = np.array([res]) # correct format to feed the vectors to sklearn clustering methods\n",
    "        \n",
    "    return res\n",
    "\n",
    "\n",
    "def train_cluster_model(\n",
    "    postproc_roles,\n",
    "    model: Union[USE, SIF_word2vec, SIF_keyed_vectors],\n",
    "    n_clusters,\n",
    "    UsedRoles = List[str],\n",
    "    random_state: Optional[int] = 0\n",
    "):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    A function to train a kmeans model on the corpus.\n",
    "    \n",
    "    Args:\n",
    "        postproc_roles: list of statements\n",
    "        model: trained embedding model \n",
    "        (e.g. either Universal Sentence Encoders, a full gensim Word2Vec model or gensim Keyed Vectors)\n",
    "        n_clusters: number of clusters\n",
    "        UsedRoles: dict with the specifics of the pipeline for each role\n",
    "        random_state: seed for replication (default is 0)\n",
    "        \n",
    "    Returns:\n",
    "        A sklearn kmeans model\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    role_counts = get_role_counts(postproc_roles, roles = UsedRoles) \n",
    "\n",
    "    role_counts = [role.split() for role in list(role_counts)]\n",
    "\n",
    "    vecs = None\n",
    "    for role in role_counts:\n",
    "        if vecs is None:\n",
    "            vecs = get_vector(role, model)\n",
    "        else:\n",
    "            temp = get_vector(role, model)\n",
    "            if temp is not None:\n",
    "                vecs = np.concatenate((vecs, temp), axis=0)\n",
    "        \n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state).fit(vecs)\n",
    "    \n",
    "    return kmeans\n",
    "\n",
    "\n",
    "def get_clusters(\n",
    "    postproc_roles: List[dict],\n",
    "    model: Union[USE, SIF_word2vec, SIF_keyed_vectors],\n",
    "    kmeans,\n",
    "    UsedRoles = List[str]\n",
    ") -> List[dict]:\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    A function which predicts clusters based on a pre-trained kmeans model.\n",
    "    \n",
    "    Args:\n",
    "        postproc_roles: list of statements\n",
    "        model: trained embedding model \n",
    "        (e.g. either Universal Sentence Encoders, a full gensim Word2Vec model or gensim Keyed Vectors)\n",
    "        kmeans = a pre-trained sklearn kmeans model\n",
    "        UsedRoles: dict with the specifics of the pipeline for each role\n",
    "        \n",
    "    Returns:\n",
    "        A list of dictionaries with the predicted cluster for each role\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    clustering_res = []\n",
    "    \n",
    "    for statement in postproc_roles:\n",
    "        temp = {}\n",
    "        for role, tokens in statement.items():\n",
    "            if role in UsedRoles:\n",
    "                vec = get_vector(tokens, model)\n",
    "                if vec is not None:\n",
    "                    clu = kmeans.predict(vec)\n",
    "                    temp[role] = int(clu)\n",
    "        clustering_res = clustering_res + [temp]\n",
    "\n",
    "    return clustering_res\n",
    "\n",
    "\n",
    "def label_clusters_most_freq(\n",
    "    clustering_res: List[dict],\n",
    "    postproc_roles: List[dict]\n",
    ") -> dict:\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    A function which labels clusters by their most frequent term.\n",
    "    \n",
    "    Args:\n",
    "        clustering_res: list of dictionaries with the predicted cluster for each role\n",
    "        postproc_roles: list of statements\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary associating to each cluster number a label (e.g. the most frequent term in this cluster)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    temp = {}\n",
    "    labels = {}\n",
    "\n",
    "    for i,statement in enumerate(clustering_res):\n",
    "        for role, cluster in statement.items():\n",
    "            tokens = ' '.join(postproc_roles[i][role])\n",
    "            cluster_num = cluster\n",
    "            if cluster_num not in temp:\n",
    "                temp[cluster_num] = [tokens]\n",
    "            else:\n",
    "                temp[cluster_num] = temp[cluster_num] + [tokens]\n",
    "\n",
    "    for cluster_num, tokens in temp.items():\n",
    "        token_counts = Counter(tokens)\n",
    "        token_freq = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        most_freq_token = token_freq[0][0]\n",
    "        labels[cluster_num] = most_freq_token\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "# Wrappers\n",
    "#..................................................................................................................\n",
    "#..................................................................................................................\n",
    "\n",
    "\n",
    "def build_narratives(\n",
    "    final_statements,\n",
    "    narrative_model: dict,\n",
    "    filter_complete_narratives: Optional[bool] = True\n",
    "):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    A function to make columns of 'raw' and 'cleaned' narratives.\n",
    "    \n",
    "    Args:\n",
    "        final_statements: dataframe with the output of the pipeline \n",
    "        narrative_model: dict with the specifics of the narrative model\n",
    "        filter_complete_narratives: keep only narratives with at least an agent, a verb and a patient \n",
    "        (default is True)\n",
    "        \n",
    "    Returns:\n",
    "        A pandas dataframe with the resulting narratives and two additional columns: \n",
    "        narrative-RAW and narrative-CLEANED \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    narrative_format = [str(role + '-RAW') for role in narrative_model['roles_considered']]\n",
    "    \n",
    "    final_statements = final_statements.replace({'': np.NaN})\n",
    "    \n",
    "    if filter_complete_narratives:\n",
    "        list_for_filter = [\n",
    "            arg for arg in narrative_format if arg not in [\n",
    "                'ARG2-RAW',\n",
    "                'B-ARGM-NEG-RAW',\n",
    "                'B-ARGM-MOD-RAW'\n",
    "            ]\n",
    "        ]\n",
    "        final_statements = final_statements.dropna(subset=list_for_filter)\n",
    "        \n",
    "    final_statements = final_statements.replace({np.NaN: ''})\n",
    "    final_statements = final_statements.replace({True: 'not'})\n",
    "    \n",
    "    final_statements['narrative-RAW'] = final_statements[narrative_format].agg(' '.join, axis=1)\n",
    "    final_statements['narrative-RAW'] = final_statements['narrative-RAW'].apply(remove_extra_whitespaces)\n",
    "\n",
    "    narrative_format = []\n",
    "    for role in narrative_model['roles_considered']:\n",
    "        if role == 'B-V':\n",
    "            if narrative_model['dimension_reduce_verbs'] == True:\n",
    "                narrative_format = narrative_format + ['B-V-CLEANED']\n",
    "                narrative_format = narrative_format + ['B-ARGM-NEG-CLEANED']\n",
    "            else:\n",
    "                narrative_format = narrative_format + ['B-V-RAW']\n",
    "                narrative_format = narrative_format + ['B-ARGM-NEG-RAW']\n",
    "        \n",
    "        elif role == 'B-ARGM-NEG':\n",
    "            continue\n",
    "        \n",
    "        elif role == 'B-ARGM-MOD':\n",
    "            narrative_format = narrative_format + ['B-ARGM-MOD-RAW']\n",
    "        \n",
    "        else:\n",
    "            if narrative_model['roles_with_embeddings'] is not None or narrative_model['roles_with_entities'] is not None:\n",
    "                narrative_format = narrative_format + [role]\n",
    "            else:\n",
    "                narrative_format = narrative_format + [str(role + '-RAW')]\n",
    "    \n",
    "    final_statements['narrative-CLEANED'] = final_statements[narrative_format].agg(' '.join, axis=1)\n",
    "    final_statements['narrative-CLEANED'] = final_statements['narrative-CLEANED'].apply(remove_extra_whitespaces)\n",
    "        \n",
    "    return final_statements\n",
    "\n",
    "\n",
    "def run_srl(\n",
    "    path: str, \n",
    "    sentences: List[str],\n",
    "    max_batch_char_length: Optional[int] = None,\n",
    "    batch_size: Optional[int] = None,\n",
    "    max_sentence_length: Optional[int] = None,\n",
    "    max_number_words: Optional[int] = None,\n",
    "    cuda_empty_cache: bool = None,\n",
    "    cuda_sleep: float = None,\n",
    "    save_to_disk: Optional[str] = None\n",
    "):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    A wrapper function to run semantic role labeling on a corpus.\n",
    "    \n",
    "    Args:\n",
    "        path: location of the SRL model to be used \n",
    "        sentences: list of sentences\n",
    "        SRL_options: see class SRL()\n",
    "        save_to_disk: path to save the narrative model (default is None, which means no saving to disk)\n",
    "        \n",
    "    Returns:\n",
    "        A list of dictionaries with the SRL output\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print('Loading SRL model...')\n",
    "    srl = SRL(path=path)\n",
    "    \n",
    "    print('Running semantic role labeling...')\n",
    "    time.sleep(1)\n",
    "    srl_res = srl(sentences=sentences, batch_size = batch_size)\n",
    "    \n",
    "    if save_to_disk is not None:\n",
    "        with open(save_to_disk, 'w') as json_file:\n",
    "            json.dump(srl_res, json_file)\n",
    "    \n",
    "    return srl_res\n",
    "\n",
    "\n",
    "def build_narrative_model(\n",
    "    srl_res: List[dict],\n",
    "    sentences: List[str],\n",
    "    roles_considered: Optional[List[str]] = ['ARGO', 'B-V', 'B-ARGM-NEG', 'B-ARGM-MOD', 'ARG1', 'ARG2'],\n",
    "    save_to_disk: Optional[str] = None,\n",
    "    max_length: Optional[int] = None,\n",
    "    remove_punctuation: Optional[bool] = True,\n",
    "    remove_digits: Optional[bool] = True,\n",
    "    remove_chars: Optional[str] = \"\",\n",
    "    stop_words: Optional[List[str]] = None,\n",
    "    lowercase: Optional[bool] = True,\n",
    "    strip: Optional[bool] = True,\n",
    "    remove_whitespaces: Optional[bool] = True,\n",
    "    lemmatize: Optional[bool] = False,\n",
    "    stem: Optional[bool] = False,\n",
    "    tags_to_keep: Optional[List[str]] = None,\n",
    "    remove_n_letter_words: Optional[int] = None,\n",
    "    roles_with_embeddings: Optional[List[str]] = ['ARGO', 'ARG1', 'ARG2'], \n",
    "    embeddings_type: Optional[str] = None,\n",
    "    embeddings_path: Optional[str] = None,\n",
    "    n_clusters: Optional[int] = 0,\n",
    "    roles_with_entities: Optional[List[str]] = ['ARGO', 'ARG1', 'ARG2'],\n",
    "    ent_labels: Optional[List[str]] = ['PERSON', 'NORP', 'ORG', 'GPE', 'EVENT'],\n",
    "    top_n_entities: Optional[int] = 0,\n",
    "    dimension_reduce_verbs: Optional[bool] = True\n",
    "):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    A wrapper function to build the narrative model from a sample of of the corpus.\n",
    "    \n",
    "    Args:\n",
    "        UsedRoles: dict with the specifics of the pipeline for each role\n",
    "        srl_res: sentences labeled with their semantic roles \n",
    "        sentences: list of sentences\n",
    "        save_to_disk: path to save the narrative model (default is None, which means no saving to disk)\n",
    "        embeddings_type: whether the user wants to use USE / Keyed Vectors or a custom pre-trained Word2Vec\n",
    "        (e.g. \"USE\" / \"gensim_keyed_vectors\" / \"gesim_full_model\")\n",
    "        embeddings_path: path for the trained embeddings model\n",
    "        n_clusters: number of clusters for the clustering model\n",
    "        preprocessing_options: see preprocess() function\n",
    "        ent_labels: list of entity labels to be considered (see SPaCy documentation)\n",
    "        top_n_entities: number of named entities to keep (default is all and is specified with top_n = 0)\n",
    "        \n",
    "        \n",
    "    Returns:\n",
    "        A tuple object to extract narratives from text\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Sanity checks\n",
    "    if is_subsequence(roles_considered, ['ARGO', 'B-V', 'B-ARGM-NEG', 'B-ARGM-MOD', 'ARG1', 'ARG2']) == False:\n",
    "        raise ValueError(\"Some roles_considered do not exist.\")\n",
    "        \n",
    "    if is_subsequence(['ARGO', 'B-V', 'B-ARGM-NEG', 'ARG1'], roles_considered) == False:\n",
    "        raise ValueError(\"Minimum roles to consider: ['ARGO', 'B-V', 'B-ARGM-NEG', 'ARG1']\")\n",
    "        \n",
    "    if roles_with_entities is not None:\n",
    "        if is_subsequence(roles_with_entities, roles_considered) == False:\n",
    "            raise ValueError(\"roles_with_entities should be in roles_considered.\")\n",
    "        \n",
    "    if roles_with_embeddings is not None:\n",
    "        if is_subsequence(roles_with_embeddings, roles_considered) == False:\n",
    "            raise ValueError(\"roles_with_embeddings should be in roles_considered.\")\n",
    "      \n",
    "    if roles_with_embeddings is not None:\n",
    "        if embeddings_type not in ['gensim_keyed_vectors', 'gensim_full_model', 'USE']:\n",
    "            raise TypeError(\"Only three types of embeddings accepted: gensim_keyed_vectors, gensim_full_model, USE\")\n",
    "\n",
    "    if is_subsequence(ent_labels, ['PERSON', 'NORP', 'ORG', 'GPE', 'EVENT']) == False:\n",
    "        raise ValueError(\"Some ent_labels do not exist.\")\n",
    "            \n",
    "    if lemmatize is True and stem is True:\n",
    "        raise ValueError(\"lemmatize and stemming cannot be both True\")\n",
    "    \n",
    "    # Narrative model dictionary \n",
    "    \n",
    "    narrative_model = {}\n",
    "    \n",
    "    narrative_model['roles_considered'] = roles_considered\n",
    "    narrative_model['roles_with_entities'] = roles_with_entities\n",
    "    narrative_model['roles_with_embeddings'] = roles_with_embeddings\n",
    "    narrative_model['dimension_reduce_verbs'] = dimension_reduce_verbs\n",
    "    narrative_model['clean_text_options'] = {\n",
    "        'max_length': max_length,\n",
    "        'remove_punctuation': remove_punctuation,\n",
    "        'remove_digits': remove_digits,\n",
    "        'remove_chars': remove_chars,\n",
    "        'stop_words': stop_words,\n",
    "        'lowercase': lowercase,\n",
    "        'strip': strip,\n",
    "        'remove_whitespaces': remove_whitespaces,\n",
    "        'lemmatize': lemmatize,\n",
    "        'stem': stem,\n",
    "        'tags_to_keep': tags_to_keep,\n",
    "        'remove_n_letter_words': remove_n_letter_words\n",
    "    }\n",
    "    \n",
    "    # Process SRL\n",
    "    print('Processing srl output...')\n",
    "    roles, sentence_index = extract_roles(srl_res, UsedRoles = roles_considered)\n",
    "    postproc_roles = postprocess_roles(roles,\n",
    "                                       max_length,\n",
    "                                       remove_punctuation,\n",
    "                                       remove_digits,\n",
    "                                       remove_chars,\n",
    "                                       stop_words,\n",
    "                                       lowercase,\n",
    "                                       strip,\n",
    "                                       remove_whitespaces,\n",
    "                                       lemmatize,\n",
    "                                       stem,\n",
    "                                       tags_to_keep,\n",
    "                                       remove_n_letter_words)\n",
    "    \n",
    "    # Verb Counts\n",
    "    if dimension_reduce_verbs:\n",
    "        print('Counting verbs...')\n",
    "        time.sleep(1)\n",
    "        verb_counts = get_role_counts(postproc_roles, roles = ['B-V'])\n",
    "        narrative_model['verb_counts'] = verb_counts\n",
    "    \n",
    "    # Named Entities\n",
    "    if roles_with_entities is not None:\n",
    "        print('Processing named entities...')\n",
    "        entities_sorted = mine_entities(sentences, ent_labels = ent_labels)\n",
    "        entities = pick_top_entities(entities_sorted, top_n_entities = top_n_entities)\n",
    "        entity_index, postproc_roles = map_entities(statements = postproc_roles,\n",
    "                                                    entities = entities,\n",
    "                                                    UsedRoles = roles_with_entities)\n",
    "        narrative_model['entities'] = entities\n",
    "        \n",
    "    # Embeddings and clustering\n",
    "    if roles_with_embeddings is not None:\n",
    "        print('Loading embeddings model...')\n",
    "        sentences = preprocess(sentences,\n",
    "                               remove_punctuation,\n",
    "                               remove_digits,\n",
    "                               remove_chars,\n",
    "                               stop_words,\n",
    "                               lowercase,\n",
    "                               strip,\n",
    "                               remove_whitespaces,\n",
    "                               lemmatize,\n",
    "                               stem,\n",
    "                               tags_to_keep,\n",
    "                               remove_n_letter_words)\n",
    "\n",
    "        if embeddings_type == 'gensim_keyed_vectors':\n",
    "            model = SIF_keyed_vectors(path = embeddings_path, sentences = sentences)\n",
    "        if embeddings_type == 'gensim_full_model':\n",
    "            model = SIF_word2vec(path = embeddings_path, sentences = sentences)\n",
    "        if embeddings_type == 'USE':\n",
    "            model = USE(path = embeddings_path)\n",
    "\n",
    "        print('Clustering remaining arguments...')\n",
    "        time.sleep(1)\n",
    "        \n",
    "        if n_clusters == 0:\n",
    "            test = list(get_role_counts(postproc_roles, roles = roles_with_embeddings))\n",
    "            n_clusters = round(len(test)/100)\n",
    "            print('Number of clusters eventually chosen: %s' %n_clusters)\n",
    "        \n",
    "        kmeans = train_cluster_model(postproc_roles, \n",
    "                                     model, \n",
    "                                     n_clusters = n_clusters, \n",
    "                                     UsedRoles=roles_with_embeddings)\n",
    "\n",
    "        clustering_res = get_clusters(postproc_roles, \n",
    "                                      model, \n",
    "                                      kmeans, \n",
    "                                      UsedRoles=roles_with_embeddings)\n",
    "\n",
    "        labels = label_clusters_most_freq(clustering_res=clustering_res, \n",
    "                                          postproc_roles=postproc_roles)\n",
    "    \n",
    "        narrative_model['embeddings_model'] = model\n",
    "        narrative_model['cluster_model'] = kmeans\n",
    "        narrative_model['cluster_labels'] = labels\n",
    "    \n",
    "    if save_to_disk is not None:\n",
    "        with open(save_to_disk, 'wb') as f:\n",
    "            pk.dump(narrative_model, f)\n",
    "                    \n",
    "    return narrative_model\n",
    " \n",
    "    \n",
    "def get_narratives(\n",
    "    srl_res: List[dict],\n",
    "    doc_index: List[int],\n",
    "    narrative_model: dict,\n",
    "    filter_complete_narratives: Optional[bool] = True\n",
    "):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    A wrapper function to obtain the final mined narratives.\n",
    "    \n",
    "    Args:\n",
    "        srl_res: sentences labeled with their semantic roles \n",
    "        doc_index: list of indices to keep track of original documents\n",
    "        narrative_model: dict with the specifics of the narrative model\n",
    "        \n",
    "    Returns:\n",
    "        A pandas dataframe with the resulting narratives\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    final_statements = []    \n",
    "    \n",
    "    # Process SRL\n",
    "    print('Processing srl output...')\n",
    "    roles, sentence_index = extract_roles(srl_res, UsedRoles = narrative_model['roles_considered'])\n",
    "    postproc_roles = postprocess_roles(roles,\n",
    "                                       narrative_model['clean_text_options']['max_length'],\n",
    "                                       narrative_model['clean_text_options']['remove_punctuation'],\n",
    "                                       narrative_model['clean_text_options']['remove_digits'],\n",
    "                                       narrative_model['clean_text_options']['remove_chars'],\n",
    "                                       narrative_model['clean_text_options']['stop_words'],\n",
    "                                       narrative_model['clean_text_options']['lowercase'],\n",
    "                                       narrative_model['clean_text_options']['strip'],\n",
    "                                       narrative_model['clean_text_options']['remove_whitespaces'],\n",
    "                                       narrative_model['clean_text_options']['lemmatize'],\n",
    "                                       narrative_model['clean_text_options']['stem'],\n",
    "                                       narrative_model['clean_text_options']['tags_to_keep'],\n",
    "                                       narrative_model['clean_text_options']['remove_n_letter_words'])\n",
    "    \n",
    "    for statement in postproc_roles:\n",
    "        temp = {}\n",
    "        for role, tokens in statement.items():\n",
    "            name = role + '-RAW'\n",
    "            if type(tokens)!=bool:\n",
    "                temp[name] = ' '.join(tokens)\n",
    "            else:\n",
    "                temp[name] = tokens\n",
    "        final_statements = final_statements + [temp]\n",
    "    \n",
    "    # Dimension reduction of verbs \n",
    "    if narrative_model['dimension_reduce_verbs']:\n",
    "        print('Dimension reduction of verbs...')\n",
    "        cleaned_verbs = clean_verbs(postproc_roles, narrative_model['verb_counts']) \n",
    "    \n",
    "        for i,statement in enumerate(cleaned_verbs):\n",
    "            for role, value in statement.items():\n",
    "                final_statements[i][role] = value\n",
    "    \n",
    "    # Named Entities\n",
    "    if narrative_model['roles_with_entities'] is not None:\n",
    "        print('Processing named entities...')\n",
    "        entity_index, postproc_roles = map_entities(statements = postproc_roles,\n",
    "                                                                     entities = narrative_model['entities'],\n",
    "                                                                     UsedRoles = narrative_model['roles_with_entities'])\n",
    "    \n",
    "        for role in narrative_model['roles_with_entities']: \n",
    "            for token, indices in entity_index[role].items():\n",
    "                for index in indices:\n",
    "                    final_statements[index][role] = token\n",
    "    \n",
    "    # Embeddings\n",
    "    if narrative_model['roles_with_embeddings'] is not None:\n",
    "        print('Clustering remaining arguments...')\n",
    "        clustering_res = get_clusters(postproc_roles, \n",
    "                                      narrative_model['embeddings_model'], \n",
    "                                      narrative_model['cluster_model'], \n",
    "                                      UsedRoles=narrative_model['roles_with_embeddings'])\n",
    "    \n",
    "        for i,statement in enumerate(clustering_res):\n",
    "            for role, cluster in statement.items():\n",
    "                final_statements[i][role] = narrative_model['cluster_labels'][cluster]\n",
    "        \n",
    "    # Original sentence and document\n",
    "    for i,index in enumerate(sentence_index):\n",
    "        final_statements[i]['sentence'] = index\n",
    "        final_statements[i]['doc'] = doc_index[index]\n",
    "        \n",
    "    final_statements = pd.DataFrame(final_statements)\n",
    "    final_statements['statement'] = final_statements.index\n",
    "    \n",
    "    final_statements = build_narratives(final_statements, narrative_model, filter_complete_narratives)\n",
    "                \n",
    "    return final_statements\n",
    "\n",
    "\n",
    "# Analysis\n",
    "#..................................................................................................................\n",
    "#..................................................................................................................\n",
    "\n",
    "\n",
    "def inspect_label(\n",
    "    final_statements,\n",
    "    label: str,\n",
    "    role: str\n",
    "):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    A function to inspect the content of a label for a user-specified role \n",
    "    (i.e. to check the 'quality' of the clusters and named entity recognition).\n",
    "    \n",
    "    Args:\n",
    "        final_statements: dataframe with the output of the pipeline \n",
    "        label: label to inspect\n",
    "        role: role to inspect\n",
    "        \n",
    "    Returns:\n",
    "        A pandas series sorted by frequency of raw roles contained in this label\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    res = final_statements.loc[final_statements[role] == label, str(role + '-RAW')].value_counts()\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "def inspect_narrative(\n",
    "    final_statements,\n",
    "    narrative: str\n",
    "):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    A function to inspect the raw statements represented by a narrative \n",
    "    (i.e. to check the 'quality' of the final narratives).\n",
    "    \n",
    "    Args:\n",
    "        final_statements: dataframe with the output of the pipeline \n",
    "        narrative: cleaned narrative to inspect\n",
    "        \n",
    "    Returns:\n",
    "        A pandas series sorted by frequency of raw narratives contained in this label\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    res = final_statements.loc[final_statements['narrative-CLEANED'] == narrative, 'narrative-RAW'].value_counts()\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-dream",
   "metadata": {},
   "source": [
    "### Pipeline in Action (with keyed vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hourly-basement",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('trump_tweets_cleaned.csv')\n",
    "train_data = df.iloc[0:100]\n",
    "\n",
    "stop_words = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rental-bernard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting into sentences...\n",
      "Loading SRL model...\n",
      "Running semantic role labeling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:20<00:00,  6.69s/it]\n"
     ]
    }
   ],
   "source": [
    "split_sentences = split_into_sentences(train_data)\n",
    "\n",
    "srl_res = run_srl(path = \"../srl-model-2018.05.25.tar.gz\",\n",
    "                  sentences=split_sentences[1],\n",
    "                  save_to_disk = 'srl_res.json',\n",
    "                  batch_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "complete-custody",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing srl output...\n",
      "Counting verbs...\n",
      "Processing named entities...\n",
      "Loading embeddings model...\n",
      "Clustering remaining arguments...\n",
      "Number of clusters eventually chosen: 1\n"
     ]
    }
   ],
   "source": [
    "narrative_model = build_narrative_model(srl_res = srl_res, \n",
    "                                        embeddings_type = \"gensim_keyed_vectors\",\n",
    "                                        embeddings_path = \"glove-wiki-gigaword-300\",\n",
    "                                        #roles_considered = ['ARGO', 'B-V', 'B-ARGM-NEG', 'B-ARGM-MOD', 'ARG1'],\n",
    "                                        #roles_with_embeddings = ['ARGO'],\n",
    "                                        #roles_with_entities = ['ARG1', 'ARG2'],\n",
    "                                        #dimension_reduce_verbs = False,\n",
    "                                        n_clusters = 0,\n",
    "                                        sentences = split_sentences[1],\n",
    "                                        save_to_disk = 'narrative_model.pk',\n",
    "                                        max_length = 4,\n",
    "                                        remove_punctuation = True,\n",
    "                                        remove_digits = True,\n",
    "                                        remove_chars = '',\n",
    "                                        stop_words = stop_words,\n",
    "                                        lowercase = True,\n",
    "                                        strip = True,\n",
    "                                        remove_whitespaces = True,\n",
    "                                        lemmatize = True,\n",
    "                                        stem = False,\n",
    "                                        tags_to_keep = None,\n",
    "                                        remove_n_letter_words = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "thermal-money",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing srl output...\n",
      "Processing named entities...\n",
      "Clustering remaining arguments...\n",
      "['ARGO-RAW', 'B-V-RAW', 'ARG1-RAW']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B-V-RAW</th>\n",
       "      <th>sentence</th>\n",
       "      <th>doc</th>\n",
       "      <th>ARGO-RAW</th>\n",
       "      <th>ARG1-RAW</th>\n",
       "      <th>ARGO</th>\n",
       "      <th>ARG1</th>\n",
       "      <th>B-ARGM-NEG-RAW</th>\n",
       "      <th>B-ARGM-MOD-RAW</th>\n",
       "      <th>statement</th>\n",
       "      <th>narrative-RAW</th>\n",
       "      <th>narrative-CLEANED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>create</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>republican democrat</td>\n",
       "      <td>economic problem</td>\n",
       "      <td>democrat</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>republican democrat create economic problem</td>\n",
       "      <td>democrat create</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>love</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>hardworking american patriot</td>\n",
       "      <td>country</td>\n",
       "      <td>hardworking american patriot</td>\n",
       "      <td>country</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>6</td>\n",
       "      <td>hardworking american patriot love country</td>\n",
       "      <td>hardworking american patriot love country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cherish</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>hardworking american patriot</td>\n",
       "      <td>value</td>\n",
       "      <td>hardworking american patriot</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>hardworking american patriot cherish value</td>\n",
       "      <td>hardworking american patriot cherish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>respect</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>hardworking american patriot</td>\n",
       "      <td>law</td>\n",
       "      <td>hardworking american patriot</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>8</td>\n",
       "      <td>hardworking american patriot respect law</td>\n",
       "      <td>hardworking american patriot respect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>put</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>hardworking american patriot</td>\n",
       "      <td>america first</td>\n",
       "      <td>hardworking american patriot</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>9</td>\n",
       "      <td>hardworking american patriot put america first</td>\n",
       "      <td>hardworking american patriot put</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>use</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>almost recent election</td>\n",
       "      <td>system</td>\n",
       "      <td>hardworking american patriot</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>14</td>\n",
       "      <td>almost recent election use system</td>\n",
       "      <td>hardworking american patriot use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>agree</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>sudan</td>\n",
       "      <td>peace normalization agreement israel</td>\n",
       "      <td>hardworking american patriot</td>\n",
       "      <td>israel</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>43</td>\n",
       "      <td>sudan agree peace normalization agreement israel</td>\n",
       "      <td>hardworking american patriot agree israel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>look</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>wisconsin</td>\n",
       "      <td>good</td>\n",
       "      <td>democrat</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>60</td>\n",
       "      <td>wisconsin look good</td>\n",
       "      <td>democrat look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>look</td>\n",
       "      <td>55</td>\n",
       "      <td>24</td>\n",
       "      <td>poll number</td>\n",
       "      <td>strong</td>\n",
       "      <td>democrat</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>113</td>\n",
       "      <td>poll number look strong</td>\n",
       "      <td>democrat look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>run</td>\n",
       "      <td>74</td>\n",
       "      <td>33</td>\n",
       "      <td>dominion</td>\n",
       "      <td>election</td>\n",
       "      <td>hardworking american patriot</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>138</td>\n",
       "      <td>dominion run election</td>\n",
       "      <td>hardworking american patriot run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>say</td>\n",
       "      <td>81</td>\n",
       "      <td>36</td>\n",
       "      <td>biden</td>\n",
       "      <td>terrible thing</td>\n",
       "      <td>democrat</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>146</td>\n",
       "      <td>biden say terrible thing</td>\n",
       "      <td>democrat say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>announce</td>\n",
       "      <td>88</td>\n",
       "      <td>39</td>\n",
       "      <td>others</td>\n",
       "      <td>vaccine</td>\n",
       "      <td>hardworking american patriot</td>\n",
       "      <td>vaccine</td>\n",
       "      <td></td>\n",
       "      <td>would</td>\n",
       "      <td>156</td>\n",
       "      <td>others announce would vaccine</td>\n",
       "      <td>hardworking american patriot announce would va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>destroyed</td>\n",
       "      <td>101</td>\n",
       "      <td>43</td>\n",
       "      <td>bureaucracy</td>\n",
       "      <td>million life</td>\n",
       "      <td>hardworking american patriot</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>would</td>\n",
       "      <td>196</td>\n",
       "      <td>bureaucracy destroyed would million life</td>\n",
       "      <td>hardworking american patriot destroyed would</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>end</td>\n",
       "      <td>108</td>\n",
       "      <td>46</td>\n",
       "      <td>great discovery</td>\n",
       "      <td>china plague</td>\n",
       "      <td>hardworking american patriot</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>214</td>\n",
       "      <td>great discovery end china plague</td>\n",
       "      <td>hardworking american patriot end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>mean</td>\n",
       "      <td>109</td>\n",
       "      <td>47</td>\n",
       "      <td>fake recount go georgia</td>\n",
       "      <td>nothing</td>\n",
       "      <td>democrat</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>217</td>\n",
       "      <td>fake recount go georgia mean nothing</td>\n",
       "      <td>democrat mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>reject</td>\n",
       "      <td>117</td>\n",
       "      <td>52</td>\n",
       "      <td>vice president</td>\n",
       "      <td>fraudulently chosen elector</td>\n",
       "      <td>democrat</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>232</td>\n",
       "      <td>vice president reject fraudulently chosen elector</td>\n",
       "      <td>democrat reject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>expose</td>\n",
       "      <td>123</td>\n",
       "      <td>56</td>\n",
       "      <td>tweet</td>\n",
       "      <td>backlash fox news</td>\n",
       "      <td>hardworking american patriot</td>\n",
       "      <td>fox news</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>249</td>\n",
       "      <td>tweet expose backlash fox news</td>\n",
       "      <td>hardworking american patriot expose fox news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>steal</td>\n",
       "      <td>135</td>\n",
       "      <td>63</td>\n",
       "      <td>embolden radical left democrat</td>\n",
       "      <td>election victory</td>\n",
       "      <td>hardworking american patriot</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>284</td>\n",
       "      <td>embolden radical left democrat steal election ...</td>\n",
       "      <td>hardworking american patriot steal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>get</td>\n",
       "      <td>186</td>\n",
       "      <td>76</td>\n",
       "      <td>way biden</td>\n",
       "      <td>vote</td>\n",
       "      <td>democrat</td>\n",
       "      <td>vote</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>367</td>\n",
       "      <td>way biden get vote</td>\n",
       "      <td>democrat get vote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>get</td>\n",
       "      <td>219</td>\n",
       "      <td>91</td>\n",
       "      <td>andrew mccabe</td>\n",
       "      <td>totally criminal activity</td>\n",
       "      <td>democrat</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>418</td>\n",
       "      <td>andrew mccabe get totally criminal activity</td>\n",
       "      <td>democrat get</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       B-V-RAW  sentence  doc                        ARGO-RAW  \\\n",
       "1       create         0    0             republican democrat   \n",
       "6         love         1    1    hardworking american patriot   \n",
       "7      cherish         1    1    hardworking american patriot   \n",
       "8      respect         1    1    hardworking american patriot   \n",
       "9          put         1    1    hardworking american patriot   \n",
       "14         use         5    2          almost recent election   \n",
       "43       agree        19    8                           sudan   \n",
       "60        look        27   12                       wisconsin   \n",
       "113       look        55   24                     poll number   \n",
       "138        run        74   33                        dominion   \n",
       "146        say        81   36                           biden   \n",
       "156   announce        88   39                          others   \n",
       "196  destroyed       101   43                     bureaucracy   \n",
       "214        end       108   46                 great discovery   \n",
       "217       mean       109   47         fake recount go georgia   \n",
       "232     reject       117   52                  vice president   \n",
       "249     expose       123   56                           tweet   \n",
       "284      steal       135   63  embolden radical left democrat   \n",
       "367        get       186   76                       way biden   \n",
       "418        get       219   91                   andrew mccabe   \n",
       "\n",
       "                                 ARG1-RAW                          ARGO  \\\n",
       "1                        economic problem                      democrat   \n",
       "6                                 country  hardworking american patriot   \n",
       "7                                   value  hardworking american patriot   \n",
       "8                                     law  hardworking american patriot   \n",
       "9                           america first  hardworking american patriot   \n",
       "14                                 system  hardworking american patriot   \n",
       "43   peace normalization agreement israel  hardworking american patriot   \n",
       "60                                   good                      democrat   \n",
       "113                                strong                      democrat   \n",
       "138                              election  hardworking american patriot   \n",
       "146                        terrible thing                      democrat   \n",
       "156                               vaccine  hardworking american patriot   \n",
       "196                          million life  hardworking american patriot   \n",
       "214                          china plague  hardworking american patriot   \n",
       "217                               nothing                      democrat   \n",
       "232           fraudulently chosen elector                      democrat   \n",
       "249                     backlash fox news  hardworking american patriot   \n",
       "284                      election victory  hardworking american patriot   \n",
       "367                                  vote                      democrat   \n",
       "418             totally criminal activity                      democrat   \n",
       "\n",
       "         ARG1 B-ARGM-NEG-RAW B-ARGM-MOD-RAW  statement  \\\n",
       "1                                                    1   \n",
       "6     country                                        6   \n",
       "7                                                    7   \n",
       "8                                                    8   \n",
       "9                                                    9   \n",
       "14                                                  14   \n",
       "43     israel                                       43   \n",
       "60                                                  60   \n",
       "113                                                113   \n",
       "138                                                138   \n",
       "146                                                146   \n",
       "156   vaccine                         would        156   \n",
       "196                                   would        196   \n",
       "214                                                214   \n",
       "217                                                217   \n",
       "232                                                232   \n",
       "249  fox news                                      249   \n",
       "284                                                284   \n",
       "367      vote                                      367   \n",
       "418                                                418   \n",
       "\n",
       "                                         narrative-RAW  \\\n",
       "1          republican democrat create economic problem   \n",
       "6            hardworking american patriot love country   \n",
       "7           hardworking american patriot cherish value   \n",
       "8             hardworking american patriot respect law   \n",
       "9       hardworking american patriot put america first   \n",
       "14                   almost recent election use system   \n",
       "43    sudan agree peace normalization agreement israel   \n",
       "60                                 wisconsin look good   \n",
       "113                            poll number look strong   \n",
       "138                              dominion run election   \n",
       "146                           biden say terrible thing   \n",
       "156                      others announce would vaccine   \n",
       "196           bureaucracy destroyed would million life   \n",
       "214                   great discovery end china plague   \n",
       "217               fake recount go georgia mean nothing   \n",
       "232  vice president reject fraudulently chosen elector   \n",
       "249                     tweet expose backlash fox news   \n",
       "284  embolden radical left democrat steal election ...   \n",
       "367                                 way biden get vote   \n",
       "418        andrew mccabe get totally criminal activity   \n",
       "\n",
       "                                     narrative-CLEANED  \n",
       "1                                      democrat create  \n",
       "6            hardworking american patriot love country  \n",
       "7                 hardworking american patriot cherish  \n",
       "8                 hardworking american patriot respect  \n",
       "9                     hardworking american patriot put  \n",
       "14                    hardworking american patriot use  \n",
       "43           hardworking american patriot agree israel  \n",
       "60                                       democrat look  \n",
       "113                                      democrat look  \n",
       "138                   hardworking american patriot run  \n",
       "146                                       democrat say  \n",
       "156  hardworking american patriot announce would va...  \n",
       "196       hardworking american patriot destroyed would  \n",
       "214                   hardworking american patriot end  \n",
       "217                                      democrat mean  \n",
       "232                                    democrat reject  \n",
       "249       hardworking american patriot expose fox news  \n",
       "284                 hardworking american patriot steal  \n",
       "367                                  democrat get vote  \n",
       "418                                       democrat get  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_statements = get_narratives(srl_res = srl_res,\n",
    "                                  doc_index = split_sentences[0],\n",
    "                                  narrative_model = narrative_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hourly-cooling",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sents = pd.DataFrame({'doc': split_sentences[0], 'sentence': split_sentences[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "developed-diameter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hardworking american patriot    4\n",
       "great discovery                 1\n",
       "others                          1\n",
       "almost recent election          1\n",
       "vice president                  1\n",
       "Name: ARGO-RAW, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect_label(final_statements, \n",
    "              label = 'hardworking american patriot', \n",
    "              role = 'ARGO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "developmental-rochester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "republican make hardworking american patriot\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "republican democrat create economic problem    1\n",
       "Name: narrative-RAW, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect_narrative(final_statements, narrative = final_statements['narrative-CLEANED'].iloc[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
